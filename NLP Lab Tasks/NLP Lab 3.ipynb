{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPOfkH2jpRENxqrWAEztPjp"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This notebook contains a series of exercises designed to help you\n",
    "# understand and implement fundamental techniques in Natural Language Processing (NLP) feature engineering using scikit-learn.\n",
    "\n",
    "# Exercises:\n",
    "# 1.  **Bag of Words:** Learn how to transform text data into a Bag of Words representation.\n",
    "# 2.  **TF-IDF:** Explore the Term Frequency-Inverse Document Frequency (TF-IDF) method for text representation."
   ],
   "metadata": {
    "id": "YI6t3KhGYWVB",
    "ExecuteTime": {
     "end_time": "2025-11-14T12:35:17.329008Z",
     "start_time": "2025-11-14T12:35:17.327243Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a02be73"
   },
   "source": [
    "### Exercise 1: Bag of Words\n",
    "\n",
    "**Task:** Use the `CountVectorizer` from scikit-learn to transform the following text corpus into a Bag of Words representation:\n",
    "\n",
    "```python\n",
    "documents = [\n",
    "    \"Text processing is important for NLP.\",\n",
    "    \"Bag of Words is a simple text representation method.\",\n",
    "    \"Feature engineering is essential in machine learning.\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Bag of Words model represents text as an unordered collection of words, disregarding grammar and even word order, but keeping track of word frequencies. Essentially, it creates a vocabulary of all unique words in the corpus and then for each document, it counts how many times each word from the vocabulary appears."
   ],
   "metadata": {
    "id": "2vwWWEqFsQ5L"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1e15c702",
    "ExecuteTime": {
     "end_time": "2025-11-14T12:35:21.416258Z",
     "start_time": "2025-11-14T12:35:20.293478Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"Text processing is important for NLP.\",\n",
    "    \"Bag of Words is a simple text representation method.\",\n",
    "    \"Feature engineering is essential in machine learning.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text data\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to an array\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nBag of Words Array:\")\n",
    "print(bow_array)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['bag' 'engineering' 'essential' 'feature' 'for' 'important' 'in' 'is'\n",
      " 'learning' 'machine' 'method' 'nlp' 'of' 'processing' 'representation'\n",
      " 'simple' 'text' 'words']\n",
      "\n",
      "Bag of Words Array:\n",
      "[[0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1]\n",
      " [0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9387d77"
   },
   "source": [
    "### Challenge 1: Bag of Words\n",
    "\n",
    "**Task:** Using the `vectorizer` from Exercise 1, transform the following new document into its Bag of Words representation and print the result:\n",
    "\n",
    "```python\n",
    "new_document_bow = [\"NLP is an important field.\"]\n",
    "```\n",
    "\n",
    "**Hint:** You'll need to use the `transform` method on the `vectorizer` that was already fitted on the original corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8916b9b3"
   },
   "source": [
    "### Exercise 2: TF-IDF\n",
    "\n",
    "**Task:** Use the `TfidfVectorizer` from scikit-learn to transform the following text corpus into a TF-IDF representation:\n",
    "\n",
    "```python\n",
    "documents = [\n",
    "    \"Natural language processing is fun.\",\n",
    "    \"Language models are important in NLP.\",\n",
    "    \"Machine learning and NLP are closely related.\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF-IDF is a numerical statistic that reflects how important a word is to a document in a corpus. It's a product of two terms: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "- **Term Frequency (TF):** This measures how frequently a term appears in a document. The more often a word appears, the higher its TF score, implying it's important to that document.\n",
    "- **Inverse Document Frequency (IDF):** This measures how rare a term is across all documents in the corpus. Words that appear frequently in many documents (like \"the\", \"is\") will have a low IDF score, making them less important. Words that are rare across the corpus will have a high IDF score, indicating they are more distinctive to certain documents. The TF-IDF score increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. This helps to filter out common words that don't carry much meaning."
   ],
   "metadata": {
    "id": "iaKU00Pgslmv"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d954d4c6",
    "ExecuteTime": {
     "end_time": "2025-11-14T12:35:31.711672Z",
     "start_time": "2025-11-14T12:35:31.699742Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"Natural language processing is fun.\",\n",
    "    \"Language models are important in NLP.\",\n",
    "    \"Machine learning and NLP are closely related.\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to an array\n",
    "tfidf_array = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nTF-IDF Array:\")\n",
    "print(tfidf_array)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['and' 'are' 'closely' 'fun' 'important' 'in' 'is' 'language' 'learning'\n",
      " 'machine' 'models' 'natural' 'nlp' 'processing' 'related']\n",
      "\n",
      "TF-IDF Array:\n",
      "[[0.         0.         0.         0.46735098 0.         0.\n",
      "  0.46735098 0.35543247 0.         0.         0.         0.46735098\n",
      "  0.         0.46735098 0.        ]\n",
      " [0.         0.34949812 0.         0.         0.45954803 0.45954803\n",
      "  0.         0.34949812 0.         0.         0.45954803 0.\n",
      "  0.34949812 0.         0.        ]\n",
      " [0.40301621 0.30650422 0.40301621 0.         0.         0.\n",
      "  0.         0.         0.40301621 0.40301621 0.         0.\n",
      "  0.30650422 0.         0.40301621]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a874e8be"
   },
   "source": [
    "### Challenge 2: TF-IDF\n",
    "\n",
    "**Task:** Using the `vectorizer` from Exercise 2, transform the following new document into its TF-IDF representation and print the result:\n",
    "\n",
    "```python\n",
    "new_document_tfidf = [\"NLP models are important.\"]\n",
    "```\n",
    "\n",
    "**Hint:** Similar to the BoW challenge, use the `transform` method on the `vectorizer` that was already fitted on the original corpus."
   ]
  }
 ]
}
